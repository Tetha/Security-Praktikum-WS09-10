\documentclass[12pt]{article}

\title{L\\"osung des Lernproblems}

\begin{document}

\section{Die grosse Idee}
Es gibt 2 Parteien (z.B. Admins von Mailservern in Firmen), die annehmen, dass
mit einer gr\"osseren Anzahl an Emails ein Klassifikationssystem f\"ur Spam besser
trainieren kann. Deswegen wollen sie ihre bereits klassifizierten Emails
gemeinsam verwenden, um so aus einer gr\"osseren Wissensbasis einen bessere
Klassifikator zu erzeugen.
Das Problem dabei ist, dass keine der beiden Parteien etwas \"uber die Emails der
anderen Partei lernen darf. Deswegen ergebn sich zwei weitere Fragen: 
\begin{itemize}
 \item Wie kann der ID3-Algorithmus privat und verteilt berechnet werden?
 \item Wie kann der ID3-Algorithmus verwendet werden, um Emails zu klassifizieren?
\end{itemize}
\section{Private Verteilung des ID3-Algorithmus}
Der Private $ID3_{\delta}$-Algorithmus kann im ausgeteilten Paper auf Seite 192
gefunden werden. Wir sehen es als nicht sinnvoll an, diesen Algorithmus nun hier
erneut aufzuf\"uhren. Daraus folgt, dass folgende Prinzipien und Protokolle
weiter untersucht werden m\"ussen:

\begin{itemize}
 \item Yaos Protokoll, angewendet f\"ur die maximale Summe, Gleichheit und 
   die Maximum-Gain-Bestimmung
 \item das 'x*ln(x)'-Protokoll
\end{itemize}

\section{Spam-Klassifikation des ID3-Algorithmus}
Um mit dem ID3-Algorithmus Spam zu klassifizieren, ist es notwendig, Attribute
zu finden und mit diesen Attributen dann einen Entscheidungsbaum mit dem
Ergebnissen ,,Spam'' und ,,kein Spam'' zu konstruieren.

Ausgehend von ,,Learning Spam'' Ergeben sich unter anderem folgende Features:
\begin{enumerate}
\item Anzahl Vorkommnisse einzelner W\"orter hardkodiert: Diese Variante ist auf
   jeden Fall einfach zu implementieren, hat jedoch das Problem, dass sie
   einfach zu umgehen ist (es werden entweder Synonyme verwendet oder
   beispielsweise Vokale vervielfacht).
\item Verwendung aller W\"orter, die im Mail-Text vorkommen (nach einer Reduktion
   des Nachrichteninhaltes um F\"ullw\"orter): Dies sollte nicht signifikant
   komplizierter zu implementieren sein wie Variante 1 und liefert laut
   ,,Learning Spam'' relativ gute Ergebnisse.
\item Zusammenfassung \"ahnlicher W\"orter: Variante 2 hatte das Problem, dass in
   einer Email nur wenige dieser ,,high-gain''-W\"orter vorkommen, sodass die
   resultierenden Trainingsvektoren sehr leer waren. Um das zu umgehen, wurden
   W\"orter anhand von Bedeutungen zusammengefasst.
\end{enumerate}

\subsection{Bewertung}
Da das Ziel der Arbeit nicht ist, einen guten Spam-Klassifikator zu schreiben,
sondern die Aspekte der verteilten, privaten Berechnugen im Fokus stehen, denkn
wir, dass die erste oder die zweite Variante vorzuziehen sind, da sie beide
zumindest naiv einfach zu implementieren sind und zumindest Variante 2
dann relativ brauchbare Ergebnisse liefert.
In einer zweiten Ueberlegung ergibt sich jedoch, dass Variante 2 unter den
Apekten der privaten Berechnung problematisch ist: Man nehme einfach mal an,
dass einer der beiden Admins in einer Forschungsabteilung arbeitet, und es
werden mehrere wichtige Stichpunkte geheimer Forschungsergebnisse als
high-gain-Worte erkannt, die Mails sofort als 'kein Spam' klassifizieren. Daher
denken wir, es sollte f\"ur eine erste Implementierung der Arbeit ein einfaches
Z\"ahlen der Vorkomnisse von Worten ausreichen, wobei die Menge der interresanten 
Worte beispielsweise durch eine Konfigurationsdatei eingegeben werden kann.
Damit muss jedoch zuvorderst noch ein sicherer Algorithmus gefunden werden, der
nachpr\"uft, dass beide Benutzer die gleiche Wortliste verwenden. Es verbleibt
zu entscheiden, was passiert. wenn diese Wortliste nicht gleich sind. Wir denken
jedoch, dass in diesem Fall der Vorgang abgebrochen werden sollte, da zwei
ehrliche Benutzer sich vorher auf die gleiche Wortliste einigen und dann die
gleiche Wortliste verwenden und deswegen Unterschiede in der Wortliste auf einen
unehrlichen Anwender hindeuten.

\subsection{Behandlung des kontinuierlichen Attributes ,,Wortanzahl''}
Ein relativ einfacher Weg, kontinuierliche Attribute zu diskretisieren ist eine
einfache Fuzzifizierung. Wir schlage deswegen vor, dass die Anzahl Vorkommnisse
eines Wortes durch die Gesamtanzahl Worte dividiert wird, und dann folgende
Abbildung verwendet wird:
\begin{itemize}
 \item Falls dieser Wert $<$ \em{s} ist, ist der Attributwert f\"ur das Wort W ,,selten''
 \item Falls dieser Wert $>$ \em{h} ist, ist der Attributwert f\"ur das Wort W ,,h\"aufig''
 \item sonst ist der Attributwert f\"ur das Wort W ,,normal''
\end{itemize}
Die Schwellwerte \em{s} und \em{h} m\"ussen dann noch festgelegt werden und
sollten ebenfalls \"uber eine Konfigurationsdatei festlegbar sein.

\section{Weiteres Vorgehen}
Damit ergibt sich als weiteres Vorgehen, folgende Algorithmen zu verstehen
und durch kryptographische Primitive zu implementieren und dann als Vorbereitung
der Implementierung aufbereitet aufzuschreiben:
\begin{itemize}
 \item ein sicheres Protokoll, welches feststellt, ob die gleiche Wortliste
       Verwendet wird (vmtl ebenfalls ueber Yaos Protokoll)
 \item Yaos Protokoll, angewendet f\"ur die maximale Summe, Gleichheit und 
   die Maximum-Gain-Bestimmung
 \item das 'x*ln(x)'-Protokoll
\end{itemize}

\section{Referenzen}
\begin{itemize}
\item Ausgeteiltes Paper
\item Learning Spam http://www.usenix.org/events/usenix03/tech/\\
      freenix03/full\_papers/massey/massey\_html/spam.html
\end{itemize}

\end{document}
